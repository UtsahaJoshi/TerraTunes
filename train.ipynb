{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef379de-5864-40be-9231-8d974c8b0f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Ripple\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ripple\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2001' max='2001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2001/2001 3:47:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.801000</td>\n",
       "      <td>6.682422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.631100</td>\n",
       "      <td>6.678906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.518400</td>\n",
       "      <td>6.682031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor, TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the Wav2Vec2 processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Fixed audio length (e.g., 10 seconds)\n",
    "fixed_length = 10 * 16000  # 10 seconds * 16000 Hz\n",
    "\n",
    "# Custom dataset class\n",
    "class BirdSoundDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data[\"Encoded Labels\"] = self.label_encoder.fit_transform(self.data[\"Common Name\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data.iloc[idx][\"File Path\"]\n",
    "        label = self.data.iloc[idx][\"Encoded Labels\"]\n",
    "        \n",
    "        # Load and preprocess the audio file\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # Ensure the audio is mono\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Ensure the audio is exactly 10 seconds\n",
    "        if waveform.size(1) > fixed_length:\n",
    "            waveform = waveform[:, :fixed_length]\n",
    "        else:\n",
    "            padding = fixed_length - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        inputs = processor(waveform.squeeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        return {\"input_values\": inputs.input_values.squeeze(0), \"labels\": torch.tensor(label)}\n",
    "\n",
    "# Load the data to fit the label encoder\n",
    "train_df = pd.read_csv(\"dataset/train_wav.csv\")\n",
    "test_df = pd.read_csv(\"dataset/test_wav.csv\")\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "combined_df[\"Encoded Labels\"] = label_encoder.fit_transform(combined_df[\"Common Name\"])\n",
    "\n",
    "# Save the encoded labels back to the CSV files\n",
    "train_df[\"Encoded Labels\"] = label_encoder.transform(train_df[\"Common Name\"])\n",
    "test_df[\"Encoded Labels\"] = label_encoder.transform(test_df[\"Common Name\"])\n",
    "train_df.to_csv(\"dataset/train_final.csv\", index=False)\n",
    "test_df.to_csv(\"dataset/test_final.csv\", index=False)\n",
    "\n",
    "# Load the Wav2Vec2 model with the number of labels\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = BirdSoundDataset(csv_file=\"dataset/train_final.csv\")\n",
    "test_dataset = BirdSoundDataset(csv_file=\"dataset/test_final.csv\")\n",
    "\n",
    "# Custom collate function to handle padding\n",
    "def collate_fn(batch):\n",
    "    input_values = [item['input_values'].squeeze(0) for item in batch]  # Remove the channel dimension\n",
    "    labels = torch.tensor([item['labels'] for item in batch], dtype=torch.long)  # Convert labels to LongTensor\n",
    "    \n",
    "    # Pad the input values\n",
    "    input_values_padded = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
    "    return {\"input_values\": input_values_padded, \"labels\": labels}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,  # Initial learning rate\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=collate_fn,\n",
    "    optimizers=(optimizer, lr_scheduler),  # Pass the optimizer and scheduler\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "processor.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8933510-845a-4292-82b9-d53c2dae3ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and processor\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "processor.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effc8a17-c1d9-4501-b8c5-afd5508fb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8102c9-f0f8-4248-97b2-76c5b80b5aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[958, 958, 958, 958, 958, 958, 958, 958, 958, 958]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [958]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mlabel_encoder\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[1;32m---> 92\u001b[0m decoded_predictions \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(predictions)\n\u001b[0;32m     93\u001b[0m decoded_true_labels \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(true_labels)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Print the predicted and true labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:160\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    158\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n\u001b[0;32m    161\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[y]\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: [958]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the processor and the fine-tuned model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"./fine_tuned_model\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"./fine_tuned_model\").to(device)\n",
    "\n",
    "# Fixed audio length (e.g., 10 seconds)\n",
    "fixed_length = 10 * 16000  # 10 seconds * 16000 Hz\n",
    "\n",
    "# Custom dataset class\n",
    "class BirdSoundDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data[\"Encoded Labels\"] = self.label_encoder.fit_transform(self.data[\"Common Name\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data.iloc[idx][\"File Path\"]\n",
    "        label = self.data.iloc[idx][\"Encoded Labels\"]\n",
    "        \n",
    "        # Load and preprocess the audio file\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # Ensure the audio is mono\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Ensure the audio is exactly 10 seconds\n",
    "        if waveform.size(1) > fixed_length:\n",
    "            waveform = waveform[:, :fixed_length]\n",
    "        else:\n",
    "            padding = fixed_length - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        inputs = processor(waveform.squeeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        return {\"input_values\": inputs.input_values.squeeze(0), \"labels\": torch.tensor(label)}\n",
    "\n",
    "# Load the test dataset\n",
    "# Load the data to fit the label encoder\n",
    "train_df = pd.read_csv(\"dataset/train_wav.csv\")\n",
    "test_df = pd.read_csv(\"dataset/test_wav.csv\")\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "combined_df[\"Encoded Labels\"] = label_encoder.fit_transform(combined_df[\"Common Name\"])\n",
    "\n",
    "test_dataset = BirdSoundDataset(csv_file=\"dataset/test_final.csv\")\n",
    "\n",
    "# Custom collate function to handle padding\n",
    "def collate_fn(batch):\n",
    "    input_values = [item['input_values'].squeeze(0) for item in batch]  # Remove the channel dimension\n",
    "    labels = torch.tensor([item['labels'] for item in batch], dtype=torch.long)  # Convert labels to LongTensor\n",
    "    \n",
    "    # Pad the input values\n",
    "    input_values_padded = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
    "    return {\"input_values\": input_values_padded, \"labels\": labels}\n",
    "\n",
    "# Create DataLoader for test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_values).logits\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return predictions, true_labels\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions, true_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "# Decode the labels\n",
    "label_encoder = test_dataset.label_encoder\n",
    "print(predictions)\n",
    "decoded_predictions = label_encoder.inverse_transform(predictions)\n",
    "decoded_true_labels = label_encoder.inverse_transform(true_labels)\n",
    "\n",
    "# Print the predicted and true labels\n",
    "for i in range(len(decoded_true_labels)):\n",
    "    print(f\"True label: {decoded_true_labels[i]}, Predicted label: {decoded_predictions[i]}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (decoded_predictions == decoded_true_labels).mean()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Create a DataFrame to save the results\n",
    "results_df = pd.DataFrame({\n",
    "    \"File Path\": test_dataset.data[\"File Path\"],\n",
    "    \"True Label\": decoded_true_labels,\n",
    "    \"Predicted Label\": decoded_predictions\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\"test_results.csv\", index=False)\n",
    "print(\"Results saved to test_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd3ebc-6950-48e1-bd8d-f60ea7acec3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
